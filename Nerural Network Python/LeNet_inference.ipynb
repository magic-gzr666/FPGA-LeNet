{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc42076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%run LeNet_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39d8b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.float32\n",
      "tensor(-0.4242) tensor(2.6051)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 定义一个转换，将图片转换为张量，并归一化到[0, 1]范围\n",
    "transform = transforms.Compose([\n",
    "     transforms.Grayscale(num_output_channels=1),  # 确保图片是灰度的，并且有一个通道\n",
    "#     transforms.ToTensor(),  # 转换为张量，并自动归一化到[0, 1]\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 打开PNG图片并应用转换\n",
    "image_path = 'test_images/6.png'  # 替换为你的图片路径\n",
    "image = Image.open(image_path)\n",
    "tensor_image = transform(image)\n",
    "tensor_image = tensor_image.unsqueeze(0)\n",
    "\n",
    "# 打印张量的形状和数据类型\n",
    "print(tensor_image.shape)  # 应该输出 torch.Size([1, 28, 28])\n",
    "print(tensor_image.dtype)  # 应该输出 torch.float32\n",
    "print(tensor_image.min(), tensor_image.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0ccaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b424_35d7_3c28_3b32_b1d6_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b105_3a2e_3e9a_402b_3de4_aec9_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b661_b09d_3aca_3f92_3f6b_3cec_37e0_b4c0_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b695_b424_3aca_4086_3e25_346a_b5c5_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b4f4_353a_4102_3fc6_349e_b695_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b62d_2e40_3fac_404b_3506_b55d_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_285a_3d47_40e8_37e0_b424_b695_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b661_36a7_404b_4004_b105_b661_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_a454_3cab_4099_3afe_b55d_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_3710_3f37_3f92_adf8_b661_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b48c_3b18_3ffa_3c0e_b6ca_b6ca_b6ca_b3df_b30f_b528_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_3059_3df1_3f5e_ac57_b695_b2a7_2d6f_38f5_39ab_3436_b3df_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_39ab_3f51_3d7b_b591_b55d_3a7c_405f_4129_4136_3fac_38db_b5f9_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_3d54_3f51_363f_b695_b1d6_404b_3c36_31fa_36db_3e66_3fb9_346a_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_af99_3e59_3f5e_349e_b4f4_3436_3e4c_2fe1_b55d_b4c0_38db_3ece_3d61_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_2e40_3ee8_3ec1_285a_b5c5_ac57_3506_b458_b6ca_b6ca_a454_3cab_4004_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_3129_3f0f_3e25_b2a7_b695_b591_b528_b661_b6ca_b6ca_b48c_3a7c_4065_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_3129_3f0f_3de4_b4f4_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b23e_3b66_4045_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_3192_3f1c_3de4_b4f4_b6ca_b6ca_b6ca_b6ca_b6ca_b695_3262_3d7b_3f02_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_2e40_3ec1_3e25_b105_b6ca_b6ca_b6ca_b6ca_b695_b1d6_3ae4_3ece_3aca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b30f_3d2d_3f37_38c1_b591_b695_b62d_b5c5_b3df_3a96_3f5e_3daf_b377_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b695_3673_3edb_4017_390f_b377_3059_38db_3edb_405f_3d06_ad28_b695_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b48c_3778_3e73_40ad_4079_4099_4093_3ece_39ab_ad28_b5f9_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b377_3192_3872_38a7_38a7_3858_3333_b1d6_b695_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_b6ca_\b\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "def float_to_float16_bin(float_num):\n",
    "    # 将十进制浮点数打包为float16格式的字节\n",
    "    packed_val = struct.pack('<e', float_num)\n",
    "    \n",
    "    # 将字节转换为整数\n",
    "    int_val = struct.unpack('<H', packed_val)[0]\n",
    "    \n",
    "    # 将整数转换为二进制字符串，并去掉前缀'0b'\n",
    "    bin_str = bin(int_val)[2:]\n",
    "    \n",
    "    # 补全到16位\n",
    "    bin_str = bin_str.zfill(16)\n",
    "    \n",
    "    return bin_str\n",
    "\n",
    "\n",
    "def binary_to_hex(binary_str):\n",
    "    # 将二进制字符串转换为整数\n",
    "    int_value = int(binary_str, 2)\n",
    "    # 将整数转换为十六进制字符串，并去掉前缀'0x'\n",
    "    hex_value = hex(int_value)[2:]\n",
    "    return hex_value\n",
    "\n",
    "\n",
    "\n",
    "for i in range(tensor_image.shape[2]):\n",
    "    for j in range(tensor_image.shape[3]):\n",
    "        # 应用函数并打印结果\n",
    "        print(binary_to_hex(float_to_float16_bin(tensor_image[0, 0, i, j].numpy())), end='_')\n",
    "print(\"\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e953f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4018e-05, 8.8574e-07, 1.2647e-05, 3.3489e-07, 1.1630e-04, 6.5211e-04,\n",
      "         9.9917e-01, 4.3490e-11, 4.3887e-06, 1.3428e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "path_to_load_weights = 'model_weights.pth'\n",
    "model.load_state_dict(torch.load(path_to_load_weights))\n",
    "output = model(tensor_image)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e816555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "max_index = output.argmax()\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3a6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
